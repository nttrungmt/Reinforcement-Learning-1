\documentclass[letterpaper,11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\begin{document}
Let us start with the defined objective function $J(\theta)$. We can expand the expectation as:

$$
\begin{aligned}
J(\theta) &= \mathbb{E}[\sum\limits_{t=0}^{T-1} r_{t+1} | \pi_{\theta}] \\ &= \sum\limits_{t=i}^{T-1}P(s_{t}, a_{t} | \tau) r_{t+1}
\end{aligned}
$$

\noindent where $i$ is an arbitrary starting point in a trajectory, $P(s_{t}, a_{t} | \tau)$ is the probability of the occurrence of $s_{t}, a_{t}$ given the
trajectory $\tau$.
\vspace{0.5cm}

\noindent Differentiate both sides with respect to policy parameter $\theta$:

$$
\text{Using  \hspace{0.3cm}}
\begin{aligned}
    \frac{d}{dx} log f(x) = \frac{f'(x)}{f(x)},
\end{aligned}
$$

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) & = \sum\limits_{t=i}^{T-1} \nabla_{\theta} P(s_{t}, a_{t} | \tau) r_{t+1} \\ 
    & = \sum\limits_{t=i}^{T-1} P(s_{t}, a_{t} | \tau) \frac{\nabla_{\theta} P(s_{t}, a_{t} | \tau)}{P(s_{t}, a_{t} | \tau)} r_{t+1} \\ 
    & = \sum\limits_{t=i}^{T-1} P(s_{t}, a_{t} | \tau) \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1} \\ 
    & = \mathbb{E} [\sum\limits_{t=i}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}]
\end{aligned}
$$

\noindent However, during, learning, we take random samples of episodes instead of computing the expectation, so we can replace the expectation with 

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) \sim \sum\limits_{t=i}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}
\end{aligned}
$$

From here, let us take a more careful look into $\nabla_{\theta} log P(s_{t}, a_{t} | \tau) $. First, by definition,

$$
\begin{aligned}
    P(s_{t}, a_{t} | \tau) ={}& P(s_{0}, a_{0}, s_{1}, a_{2}, ..., s_{t-1}, a_{t-1}, s_{t}, a_{t} | \pi_{\theta}) \\
                           ={}& P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1})
 P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) \\ & \ ... P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t} | s_{t-1}, a_{t-1}) \pi_{\theta}(a_{t} | s_{t-1})
\end{aligned}
$$

\vspace{0.2cm}
\noindent If we $log$ both sides, \\
\vspace{-0.2cm}

$$
\begin{aligned}
    log P(s_{t}, a_{t} | \tau)  ={}& log (P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1}) P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) ... \\
& P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t})log \pi_{\theta}(a_{t} | s_{t-1}) \\ ={}& log P(s_{0}) + log \pi_{\theta}(a_{1} | s_{0}) + log P(s_{1} | s_{0}, a_{0}) + log \pi_{\theta}(a_{2} | s_{1})\\
& + log P(s_{2} | s_{1}, a_{1}) + log \pi_{\theta}(a_{3} | s_{2}) + ... + log P(s_{t-1} | s_{t-2}, a_{t-2}) \\ 
 & + log \pi_{\theta}(a_{t-1} | s_{t-2}) + log P(s_{t} | s_{t-1}, a_{t-1}) + log \pi_{\theta}(a_{t} | s_{t-1})
\end{aligned}
$$

\vspace{0.2cm}
\noindent Then,  differentiating $log P(s_{t}, a_{t} | \tau)$ with respect to $\theta$ yields: \\
\vspace{-0.15cm}

$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau)  ={}& \nabla_{\theta} log P(s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log P(s_{1} | s_{0}, a_{0}) \\
& + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log P(s_{2} | s_{1}, a_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log P(s_{t-1} | s_{t-2}, a_{t-2}) + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + \\ & \nabla_{\theta} log P(s_{t} | s_{t-1}, a_{t-1}) + \nabla_{\theta} log \pi_{\theta}(a_{t} | s_{t-1})  \\
\end{aligned}
$$

\vspace{0.2cm}

However, note that the $P(s_{t} | s_{t-1}, a_{t-1})$ is not dependent on the policy parameter $\theta$, and is solely dependant on the environment on which the reinforcement learning is acting on; it is assumed that the state transition is unknown to the agent in model free reinforcement learning. Thus, the gradient of it with respect to $\theta$ will be 0. How convenient! So: 
$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau) ={}& 0 + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\ & {} ... {} + {} 0 + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + 0 \\ ={}& \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + log \pi_{\theta}(a_{t} | s_{t-1})  \\ ={}& \sum\limits_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})                                                
\end{aligned}
$$

Plugging this into our $\nabla_{\theta} J(\theta)$ yields:

$\begin{aligned}
    \nabla_{\theta} J(\theta) ={}& \sum_{t=0}^{T-1} r_{t+1} \nabla_{\theta} P(s_{t}, a_{t} | \tau) \\
                              ={}& \sum_{t=0}^{T-1} r_{t+1} (\sum_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'}))
\end{aligned}$

\vspace{1.0cm}
Lets expand that! 

$\begin{aligned}
    \nabla_{\theta} J(\theta) ={}& \sum_{t=0}^{T-1} r_{t+1} (\sum_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\
                              ={}&  r_{1} (\sum_{t'=0}^{0} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) + r_{2} (\sum_{t'=0}^{1} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\
                                 & + r_{3} (\sum_{t'=0}^{2} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) + ... + r_{T-1} (\sum_{t'=0}^{T-1} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\
                              ={}& r_{1} \nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + r_{2} (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1})) \\
                                 & + r_{3} (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) + \nabla_{\theta} log \pi_{\theta} (a_{2} | s_{2})) \\
                                 & + ... + r_{T} (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) + ... + \nabla_{\theta} log \pi_{\theta} (a_{T-1} | s_{T-1})) \\
                              ={}&  \nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) (r_{1} + r_{2} + ... + r_{T}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) (r_{2} + r_{3} + ... +  r_{T}) \\
                                 & + \nabla_{\theta} log \pi_{\theta} (a_{2} | s_{2}) (r_{3} + r_{4} + ... + r_{T}) + ... + \nabla_{\theta} log \pi_{\theta} (a_{T-1} | s_{T-1}) r_{T} \\
                              ={}& \sum_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) (\sum_{t'=t+1}^{T} r_{t'}) 
\end{aligned}$

Simplifying the term $\sum_{t'=t+1}^{T} r_{t'}$ to $G_{t}$, we can derive the policy gradient

\begin{center}
$\begin{aligned}
    \sum_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) G_{t}
\end{aligned}$
\end{center}
\vspace{0.3cm}

Incorporating the discount factor $\gamma \in [0,1]$ into our objective (in order to weight immediate rewards more than future rewards):

\begin{center}
$\begin{aligned}
    J(\theta) &= \mathbb{E}[\gamma^{0} r_{1} + \gamma^{1} r_{2} + \gamma^{2} r_{3} + ... + \gamma^{T-1} r_{T} | \pi_{\theta}]
\end{aligned}$
\end{center}

We can perform a similar derivation to obtain

\begin{center}
$\begin{aligned}
    \nabla_{\theta} J(\theta) &= \sum_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) (\sum_{t'=t+1}^{T} \gamma^{t' - t - 1} r_{t'})
\end{aligned}$
\end{center}

and simplifying $\sum_{t'=t+1}^{T} \gamma^{t' - t - 1} r_{t'}$ to $G_{t}$,

\begin{center}
$\begin{aligned}
    \nabla_{\theta} J(\theta) &= \sum_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) G_{t}
\end{aligned}$
\end{center}

\end{document}