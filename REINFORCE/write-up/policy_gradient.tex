\documentclass[letterpaper,11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\begin{document}
Let us start with the defined objective function $J(\theta)$. We can expand the expectation as:

$$
\begin{aligned}
J(\theta) &= \mathbb{E}[\sum\limits_{t=i}^{T-1} \gamma^{t-i} r_{t+1} | \pi_{\theta}] \\ &= \sum\limits_{t=i}^{T-1}P(s_{t}, a_{t} | \tau) r_{t+1}
\end{aligned}
$$

\noindent where $i$ is an arbitrary starting point in a trajectory, $P(s_{t}, a_{t} | \tau)$ is the probability of the occurrence of $s_{t}, a_{t}$ given the
trajectory $\tau$.
\vspace{0.5cm}

\noindent Differentiate both sides with respect to policy parameter $\theta$:

$$
\text{Using  \hspace{0.3cm}}
\begin{aligned}
    \frac{d}{dx} log f(x) = \frac{f'(x)}{f(x)},
\end{aligned}
$$

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) & = \sum\limits_{t=i}^{T-1} \nabla_{\theta} P(s_{t}, a_{t} | \tau) r_{t+1} \\ 
    & = \sum\limits_{t=i}^{T-1} P(s_{t}, a_{t} | \tau) \frac{\nabla_{\theta} P(s_{t}, a_{t} | \tau)}{P(s_{t}, a_{t} | \tau)} r_{t+1} \\ 
    & = \sum\limits_{t=i}^{T-1} P(s_{t}, a_{t} | \tau) \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1} \\ 
    & = \mathbb{E} [\sum\limits_{t=i}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}]
\end{aligned}
$$

\noindent However, during, learning, we take random samples of episodes instead of computing the expectation, so we can replace the expectation with 

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) \sim \sum\limits_{t=i}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}
\end{aligned}
$$

\noindent From here, let us take a more careful look into $\nabla_{\theta} log P(s_{t}, a_{t} | \tau) $.

First, by definition, representing the starting point of the sequence in a trajectory as $s_{0}$,

$$
\begin{aligned}
    P(s_{t}, a_{t} | \tau) ={}& P(s_{0}, a_{0}, s_{1}, a_{2}, ..., s_{t-1}, a_{t-1}, s_{t} | \pi_{\theta}) \\
                           ={}& P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1}) \\
& P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) ... P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t}) 
\end{aligned}
$$
\vspace{0.2cm}
\noindent If we $log$ both sides, \\
\vspace{-0.2cm}
$$
\begin{aligned}
    log P(s_{t}, a_{t} | \tau)  ={}& log (P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1}) P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) ... \\
& P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t})) \\ ={}& log P(s_{0}) + log \pi_{\theta}(a_{1} | s_{0}) + log P(s_{1} | s_{0}, a_{0}) + log \pi_{\theta}(a_{2} | s_{1})\\
& + log P(s_{2} | s_{1}, a_{1}) + log \pi_{\theta}(a_{3} | s_{2}) + ... \\
& + log P(s_{t-1} | s_{t-2}, a_{t-2}) + log \pi_{\theta}(a_{t-1} | s_{t-2}) + log P(s_{t})
\end{aligned}
$$
\vspace{0.2cm}
\noindent Then,  differentiating $log P(s_{t}, a_{t} | \tau)$ with respect to $\theta$ yields: \\
\vspace{-0.15cm}
$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau)  ={}& \nabla_{\theta} log P(s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log P(s_{1} | s_{0}, a_{0}) \\
& + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log P(s_{2} | s_{1}, a_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log P(s_{t-1} | s_{t-2}, a_{t-2}) + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + \nabla_{\theta} log P(s_{t})) \\
\end{aligned}
$$

However, note that the $P(s_{t} | s_{t-1}, a_{t-1})$ is not dependent on the policy parameter $\theta$, and is solely dependant on the environment on which the reinforcement learning is acting on; it is assumed that the state transition is unknown to the agent in model free reinforcement learning. Thus, the gradient of it with respect to $\theta$ will be 0. How convenient! 

So, 
$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau) ={}& 0 + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\ & {} ... {} + {} 0 + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + 0 \\ ={}& \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) \\ ={}& \sum\limits_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{i} | s_{i})                                                
\end{aligned}
$$

Plugging this into our $\nabla_{\theta} J(\theta)$ yields:

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) ={}& \sum\limits_{t = i}^{T - 1} \gamma^{t - i} r_{t+1} [\sum\limits_{t' = t}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})] \\
    ={}& \sum\limits_{t = i}^{T - 1} \gamma^{t - i} r_{t+1}  \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'}) \\
    ={}& \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'}) \sum\limits_{t = i}^{T - 1} \gamma^{t - i} r_{t+1} \\
    ={}& \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'}) G_{t} 
\end{aligned}
$$

\end{document}