\documentclass[letterpaper,11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\begin{document}
Let us start with the defined objective function $J(\theta)$. We can expand the expectation as:

$$
\begin{aligned}
J(\theta) &= \mathbb{E}[\sum\limits_{t=0}^{T-1} r_{t+1} | \pi_{\theta}] \\ &= \sum\limits_{t=0}^{T-1}P(s_{t}, a_{t} | \tau) R(s_{t}, a_{t}) 
\end{aligned}
$$

Differentiate both sides with respect to policy parameter $\theta$:

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) & = \sum\limits_{t=0}^{T-1} \nabla_{\theta} P(s_{t}, a_{t} | \tau) r_{t+1} \\ & = \sum\limits_{t=0}^{T-1} P(s_{t}, a_{t} | \tau) \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1} \\ & = \mathbb{E} [\sum\limits_{t=0}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}]
\end{aligned}
$$

However, during, learning, we take random samples of episodes instead of computing the expectation, so

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) \sim \sum\limits_{t=0}^{T-1} \nabla_{\theta} log P(s_{t}, a_{t} | \tau) r_{t+1}
\end{aligned}
$$

From here, let us take a more careful look into $\nabla_{\theta} log P(s_{t}, a_{t} | \tau) $.

First, by definition,

$$
\begin{aligned}
    P(s_{t}, a_{t} | \tau) ={}& P(s_{0}, a_{0}, s_{1}, a_{2}, ..., s_{t-1}, a_{t-1}, s_{t} | \pi_{\theta}) \\
                           ={}& P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1}) \\
& P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) ... P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t}) 
\end{aligned}
$$

If we $log$ both sides,

$$
\begin{aligned}
    log P(s_{t}, a_{t} | \tau)  ={}& log (P(s_{0}) \pi_{\theta}(a_{1} | s_{0}) P(s_{1} | s_{0}, a_{0}) \pi_{\theta}(a_{2} | s_{1}) P(s_{2} | s_{1}, a_{1}) \pi_{\theta}(a_{3} | s_{2}) ... \\
& P(s_{t-1} | s_{t-2}, a_{t-2}) \pi_{\theta}(a_{t-1} | s_{t-2}) P(s_{t})) \\ ={}& log P(s_{0}) + log \pi_{\theta}(a_{1} | s_{0}) + log P(s_{1} | s_{0}, a_{0}) + log \pi_{\theta}(a_{2} | s_{1})\\
& + log P(s_{2} | s_{1}, a_{1}) + log \pi_{\theta}(a_{3} | s_{2}) + ... \\
& + log P(s_{t-1} | s_{t-2}, a_{t-2}) + log \pi_{\theta}(a_{t-1} | s_{t-2}) + log P(s_{t})
\end{aligned}
$$

Then,  differentiating $log P(s_{t}, a_{t} | \tau)$ with respect to $\theta$ yields: 

$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau)  ={}& \nabla_{\theta} log P(s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log P(s_{1} | s_{0}, a_{0}) \\
& + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log P(s_{2} | s_{1}, a_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log P(s_{t-1} | s_{t-2}, a_{t-2}) + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + \nabla_{\theta} log P(s_{t})) \\
\end{aligned}
$$

However, note that the $P(s_{t} | s_{t-1}, a_{t-1})$ is not dependent on the policy parameter $\theta$, and is solely dependant on the environment on which the reinforcement learning is acting on; it is assumed that the state transition is unknown to the agent in model free reinforcement learning. Thus, the gradient of it with respect to $\theta$ will be 0. How convenient! 

So, 
$$
\begin{aligned}
    \nabla_{\theta} log P(s_{t}, a_{t} | \tau) ={}& 0 + \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + 0 + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\ & {} ... {} + {} 0 + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) + 0 \\ ={}& \nabla_{\theta} log \pi_{\theta}(a_{1} | s_{0}) + \nabla_{\theta} log \pi_{\theta}(a_{2} | s_{1}) + \nabla_{\theta} log \pi_{\theta}(a_{3} | s_{2}) + \\
& ... + \nabla_{\theta} log \pi_{\theta}(a_{t-1} | s_{t-2}) \\ ={}& \sum\limits_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})                                                
\end{aligned}
$$

Plugging this into our $\nabla_{\theta} J(\theta)$ yields:

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) ={}& \sum\limits_{t=0}^{T-1} r_{t+1} \nabla_{\theta} P(s_{t}, a_{t} | \tau) \\ ={}& \sum\limits_{t=0}^{T-1} r_{t+1} (\sum_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'}))
\end{aligned}
$$

Lets play around that with a bit. 
Say, $T = 4$. Then,

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) ={}& \sum\limits_{t=0}^{3} r_{t+1} (\sum\limits_{t'=0}^{t} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\ ={}& r_{1} (\sum\limits_{t'=0}^{0} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) + r_{2} (\sum\limits_{t'=0}^{1} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\
& + r_{3} (\sum\limits_{t'=0}^{2} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) + r_{4} (\sum\limits_{t'=0}^{3} \nabla_{\theta} log \pi_{\theta}(a_{t'} | s_{t'})) \\ ={}& r_{1} \nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + r_{2} (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1})) \\
& + r_{3}  (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) + \nabla_{\theta} log \pi_{\theta} (a_{2} | s_{2})) \\ & + r_{4} (\nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) + \nabla_{\theta} log \pi_{\theta} (a_{2} | s_{2}) \\
& + \nabla_{\theta} log \pi_{\theta} (a_{3} | s_{3})) \\ ={}&  \nabla_{\theta} log \pi_{\theta} (a_{0} | s_{0}) (r_{1} + r_{2} + r_{3} + r_{4}) \\
 & + \nabla_{\theta} log \pi_{\theta} (a_{1} | s_{1}) (r_{2} + r_{3} + r_{4}) \\ & + \nabla_{\theta} log \pi_{\theta} (a_{2} | s_{2}) (r_{3} + r_{4}) + \nabla_{\theta} log \pi_{\theta} (a_{3} | s_{3}) r_{4} \\ ={}& \sum\limits_{t=0}^{3} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) (\sum\limits_{t'=t}^{3} r_{t'+1}) 
\end{aligned}
$$

So, in general, 

$$
\begin{aligned}
\nabla_{\theta}J(\theta) = \sum\limits_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) (\sum\limits_{t'=t}^{T-1} r_{t'+1}) 
\end{aligned}
$$

Incorporating the discount factor $\gamma \in [0,1]$ for future rewards,

$$
\begin{aligned}
    \nabla_{\theta}J(\theta) = \sum\limits_{t=0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) (\sum\limits_{t'=t}^{T-1} \gamma^{t'-t} r_{t'+1}) 
\end{aligned}
$$

For simplicity, we will denote ${\sum\limits_{t'=t}^{T-1} \gamma^{t'} r_{t'-1}}$ as $G_{t}$, the discounted cumulative future reward. Replacing ${\sum\limits_{t'=t}^{T-1} \gamma^{t'-t} r_{t+1}}$ with ${G_{t}}$, we derive the policy gradient,

$$
\begin{aligned}
    \nabla_{\theta} J(\theta) = \sum_{t = 0}^{T-1} \nabla_{\theta} log \pi_{\theta} (a_{t} | s_{t}) G_{t}    
\end{aligned}
$$

Then, we update the policy paramter $\theta$ as:

$$
\begin{aligned}
    \theta \gets \theta + \alpha \nabla_{\theta} J(\theta)
\end{aligned}
$$

where $\alpha$ is the learning rate in $[0, 1]$

\end{document}